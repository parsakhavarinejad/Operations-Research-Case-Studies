{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Microgrid Dynamic Pricing with Advantage Actor-Critic",
        "\n",
        "\nThis notebook builds a minimal microgrid simulator, then trains an advantage actor-critic (A2C) policy to choose demand response actions that balance bill savings and occupant comfort. The workflow mirrors the standalone report in the repository README."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data and dependencies",
        "\n",
        "\nWe use a small household-level consumption sample with four features per timestamp: average temperature, energy use, household size, and peak-hour share. Actions represent a normalized load-shedding ratio between 0 (no reduction) and 1 (full reduction)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"household_energy_consumption.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment definition",
        "\n",
        "The environment tracks a seven-step episode sampled from the time series. Observations include normalized temperature, consumption, household size, and the derived grid price. The reward subtracts quadratic discomfort from the money saved after shedding an action ratio $a$ from the baseline load $c$ when the price is $p$:",
        "\n",
        "$$r = (c p) - (c (1 - a) p) - c a^2 \frac{T}{10}$$",
        "\n",
        "Episodes terminate after seven transitions to keep training iterations fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MicrogridEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        self.data_matrix = data[[\n",
        "            'Avg_Temperature_C',\n",
        "            'Energy_Consumption_kWh',\n",
        "            'Household_Size',\n",
        "            'Peak_Hours_Usage_kWh'\n",
        "        ]].to_numpy(dtype=np.float32)\n",
        "        self.max_vals = self.data_matrix.max(axis=0) + 1e-5\n",
        "        self.total_data_len = len(self.data_matrix)\n",
        "        self.episode_duration = 7\n",
        "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=42, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.steps_taken = 0\n",
        "        self.current_step_idx = np.random.randint(0, self.total_data_len - self.episode_duration)\n",
        "        self._load_data()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        shed_factor = float(action.item())\n",
        "        actual_cost = self.consumption * (1 - shed_factor) * self.grid_price\n",
        "        money_saved = (self.consumption * self.grid_price) - actual_cost\n",
        "        temp_factor = self.temp / 10.0\n",
        "        discomfort = self.consumption * (shed_factor ** 2) * temp_factor\n",
        "        raw_reward = money_saved - discomfort\n",
        "        self.steps_taken += 1\n",
        "        self.current_step_idx += 1\n",
        "        terminated = self.steps_taken >= self.episode_duration\n",
        "        if not terminated:\n",
        "            self._load_data()\n",
        "            new_observation = np.array([self.temp, self.consumption, self.household_size, self.grid_price], dtype=np.float32)\n",
        "        else:\n",
        "            new_observation = np.zeros(4, dtype=np.float32)\n",
        "        return new_observation, raw_reward, terminated, False, {}\n",
        "\n",
        "    def _load_data(self):\n",
        "        row = self.data_matrix[self.current_step_idx]\n",
        "        self.temp = row[0]\n",
        "        self.consumption = row[1]\n",
        "        self.household_size = row[2]\n",
        "        self.peak_usage = row[3]\n",
        "        self.grid_price = 0.12 + (0.10 * self.peak_usage)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array([self.temp, self.consumption, self.household_size, self.grid_price], dtype=np.float32)\n",
        "        obs[:3] = obs[:3] / self.max_vals[:3]\n",
        "        return obs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advantage actor-critic model",
        "\n",
        "The policy outputs a Gaussian mean $\\mu$ and standard deviation $\\sigma$ bounded to $[0, 1]$ through a sigmoid transformation. The critic estimates the state value $V(s)$. Loss combines the policy gradient with a squared value error and uses gradient clipping for stability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.actor_mu = nn.Linear(64, action_dim)\n",
        "        self.actor_sig = nn.Linear(64, action_dim)\n",
        "        self.critic = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actor_mu = torch.sigmoid(self.actor_mu(x))\n",
        "        actor_sig = torch.sigmoid(self.actor_sig(x)) * 0.3 + 0.01\n",
        "        critic_value = self.critic(x)\n",
        "        return actor_mu, actor_sig, critic_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the A2C agent",
        "\n",
        "Each episode samples a random seven-step window. For every transition, the algorithm draws an action from $\\mathcal{N}(\\mu, \\sigma)$, computes a one-step return target, and updates both actor and critic. Rewards log the combined savings and discomfort signal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "env = MicrogridEnv(df)\n",
        "input_dim = 4\n",
        "action_dim = 1\n",
        "model = ActorCriticNetwork(input_dim=input_dim, action_dim=action_dim)\n",
        "gamma = 0.99\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "all_rewards = []\n",
        "num_episodes = 2500\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    episode_reward = 0\n",
        "    terminated = False\n",
        "    while not terminated:\n",
        "        mu, sigma, value = model(state)\n",
        "        normal_dist = torch.distributions.Normal(mu, sigma)\n",
        "        action = torch.clamp(normal_dist.sample(), 0.0, 1.0)\n",
        "        next_state_np, reward, terminated, _, _ = env.step(action)\n",
        "        next_state = torch.from_numpy(next_state_np).float().unsqueeze(0)\n",
        "        episode_reward += reward\n",
        "        _, _, next_value = model(next_state)\n",
        "        target = reward if terminated else reward + gamma * next_value\n",
        "        advantage = target - value\n",
        "        critic_loss = advantage.pow(2)\n",
        "        actor_loss = -(normal_dist.log_prob(action) * advantage.detach())\n",
        "        total_loss = actor_loss + critic_loss\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        state = next_state\n",
        "    all_rewards.append(episode_reward)\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}: Reward {episode_reward:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(all_rewards)\n",
        "plt.title(\"A2C Learning Curve\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate pricing and shedding behavior",
        "\n",
        "A held-out sequence evaluates how the trained policy reacts to evolving prices. Actions follow the learned mean without exploration, and plots highlight price sensitivity and the temperature-action relationship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_start_idx = env.total_data_len - 49\n",
        "test_data = env.data_matrix[test_start_idx:]\n",
        "prices = []\n",
        "actions = []\n",
        "temps = []\n",
        "state_row = test_data[0]\n",
        "price = 0.15 + (0.20 * state_row[3])\n",
        "obs = np.array([state_row[0], state_row[1], state_row[2], price], dtype=np.float32)\n",
        "obs[:3] = obs[:3] / env.max_vals[:3]\n",
        "current_state = torch.from_numpy(obs).float().unsqueeze(0)\n",
        "print(\"Running validation simulation...\")\n",
        "for i in range(49):\n",
        "    with torch.no_grad():\n",
        "        mu, sigma, _ = model(current_state)\n",
        "        action = mu.item()\n",
        "    real_price = price\n",
        "    real_temp = state_row[0]\n",
        "    prices.append(real_price)\n",
        "    actions.append(action)\n",
        "    temps.append(real_temp)\n",
        "    if i < 48:\n",
        "        state_row = test_data[i + 1]\n",
        "        price = 0.15 + (0.20 * state_row[3])\n",
        "        obs = np.array([state_row[0], state_row[1], state_row[2], price], dtype=np.float32)\n",
        "        obs[:3] = obs[:3] / env.max_vals[:3]\n",
        "        current_state = torch.from_numpy(obs).float().unsqueeze(0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "ax1.set_xlabel(\"Validation step\")\n",
        "ax1.set_ylabel(\"Grid price ($/kWh)\", color=\"tab:red\")\n",
        "ax1.plot(prices, color=\"tab:red\", linewidth=2, label=\"Grid price\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Load shedding\", color=\"tab:blue\")\n",
        "ax2.fill_between(range(len(actions)), actions, color=\"tab:blue\", alpha=0.3, label=\"Shedding\")\n",
        "ax2.plot(actions, color=\"tab:blue\", linewidth=1)\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "ax2.set_ylim(0, 1.0)\n",
        "plt.title(\"A2C response to dynamic pricing\")\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(temps, actions, alpha=0.5, c=\"orange\")\n",
        "plt.xlabel(\"Temperature (C)\")\n",
        "plt.ylabel(\"Load shedding\")\n",
        "plt.title(\"Temperature impact on shedding\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "geo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}