{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-agent PPO for Microgrid Dynamic Pricing",
        "\n",
        "\nThis notebook scales the microgrid simulator to five parallel households and trains a decentralized Proximal Policy Optimization (PPO) agent to coordinate demand response. Agents share a policy network but experience independent discomfort and cost signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment setup",
        "\n",
        "Each observation contains normalized temperature and consumption for one household. The environment computes a neighborhood price $p = 0.1 + 0.5 \\sum_i c_i (1 - a_i)$ and assigns each agent a reward that penalizes the realized cost and quadratic discomfort."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MultiAgentMicrogridEnv(gym.Env):\n",
        "    def __init__(self, df, num_agents=5):\n",
        "        super().__init__()\n",
        "        self.num_agents = num_agents\n",
        "        unique_ids = df['Household_ID'].unique()[:num_agents]\n",
        "        df = df[df['Household_ID'].isin(unique_ids)]\n",
        "        self.temps = df.pivot(index='Date', columns='Household_ID', values='Avg_Temperature_C').fillna(20).values\n",
        "        self.cons = df.pivot(index='Date', columns='Household_ID', values='Energy_Consumption_kWh').fillna(5).values\n",
        "        self.total_step = len(df['Date'].unique())\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        t = self.temps[self.current_step] / 30.0\n",
        "        c = self.cons[self.current_step] / 10\n",
        "        return np.stack([t, c], axis=1)\n",
        "\n",
        "    def step(self, actions):\n",
        "        current_temps = self.temps[self.current_step]\n",
        "        current_cons = self.cons[self.current_step]\n",
        "        actual_load = current_cons * (1 - actions.flatten())\n",
        "        total_energy_cons = np.sum(actual_load, axis=0)\n",
        "        price = 0.1 + 0.5 * total_energy_cons\n",
        "        rewards = []\n",
        "        for agent in range(self.num_agents):\n",
        "            cost = actual_load[agent] * price\n",
        "            discomfort = current_cons[agent] * (actions[agent] ** 2) * (current_temps[agent] / 30.0)\n",
        "            reward = -(cost + discomfort)\n",
        "            rewards.append(reward)\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.total_step\n",
        "        next_obs = self._get_obs() if not done else np.zeros((self.num_agents, 2))\n",
        "        return next_obs, np.array(rewards), done, {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shared actor-critic policy",
        "\n",
        "The PPO backbone mirrors the single-agent network but exposes an `evaluate` helper for clipped objective computation. A single policy serves all agents, encouraging coordinated responses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.actor_mu = nn.Linear(64, action_dim)\n",
        "        self.actor_sig = nn.Linear(64, action_dim)\n",
        "        self.critic = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actor_mu = torch.sigmoid(self.actor_mu(x))\n",
        "        actor_sig = torch.sigmoid(self.actor_sig(x)) * 0.3 + 0.01\n",
        "        critic_value = self.critic(x)\n",
        "        return actor_mu, actor_sig, critic_value\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        mu, sig, state_value = self.forward(state)\n",
        "        dist = Normal(mu, sig)\n",
        "        actions_log_normal = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        return actions_log_normal, state_value, dist_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO update rule",
        "\n",
        "The surrogate objective clips the ratio $\frac{\\pi_\theta(a|s)}{\\pi_{\theta_{old}}(a|s)}$ within $(1 \\pm \\epsilon)$ while adding a value loss and entropy bonus to encourage exploration. Rewards are normalized per batch to stabilize learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, input_dim, action_dim, lr=0.002, gamma=0.99, K_epochs=4, eps_clip=0.2):\n",
        "        self.input_dim = input_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.K_epochs = K_epochs\n",
        "        self.eps_clip = eps_clip\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.policy = ActorCriticNetwork(input_dim, action_dim)\n",
        "        self.old_policy = ActorCriticNetwork(input_dim, action_dim)\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.buffer = []\n",
        "\n",
        "    def store(self, state, action, log_prob, reward, done):\n",
        "        obs = (state, action, log_prob, reward, done)\n",
        "        self.buffer.append(obs)\n",
        "\n",
        "    def update(self):\n",
        "        states, actions, log_probs, rewards, done = zip(*self.buffer)\n",
        "        discounted_rewards = []\n",
        "        current_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(rewards), reversed(done)):\n",
        "            if is_terminal:\n",
        "                current_reward = 0\n",
        "            current_reward = reward + self.gamma * current_reward\n",
        "            discounted_rewards.insert(0, current_reward)\n",
        "        discounted_rewards_t = torch.FloatTensor(discounted_rewards)\n",
        "        discounted_rewards_t = (discounted_rewards_t - discounted_rewards_t.mean()) / (discounted_rewards_t.std() + 1e-6)\n",
        "        old_states = torch.stack(states).squeeze().detach()\n",
        "        old_actions = torch.stack(actions).squeeze().detach()\n",
        "        old_logprobs = torch.stack(log_probs).squeeze().detach()\n",
        "        for _ in range(self.K_epochs):\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "            logprobs = logprobs.squeeze()\n",
        "            ratio = torch.exp(logprobs - old_logprobs.detach())\n",
        "            state_values = state_values.squeeze()\n",
        "            advantage = discounted_rewards_t - state_values.detach()\n",
        "            surr_1 = ratio * advantage\n",
        "            surr_2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
        "            loss = -torch.min(surr_1, surr_2) + 0.5 * F.mse_loss(state_values, discounted_rewards_t) - 0.01 * dist_entropy\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
        "        self.buffer = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-agent training loop",
        "\n",
        "Each episode collects trajectories for all agents, aggregates them into a shared buffer, and performs updates every ten episodes. The average neighborhood reward highlights the policy trend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('household_energy_consumption.csv')\n",
        "num_agents = 5\n",
        "env = MultiAgentMicrogridEnv(df, num_agents=num_agents)\n",
        "ppo_agent = PPOAgent(input_dim=2, action_dim=1, lr=0.0001, gamma=0.99)\n",
        "num_episodes = 2500\n",
        "update_interval = 10\n",
        "all_rewards = []\n",
        "print('Starting MARL PPO training...')\n",
        "agent_memories = {i: [] for i in range(num_agents)}\n",
        "for episode in range(num_episodes):\n",
        "    obs = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        actions_list = []\n",
        "        step_data = []\n",
        "        for i in range(num_agents):\n",
        "            state = torch.FloatTensor(obs[i]).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                mu, sigma, _ = ppo_agent.old_policy(state)\n",
        "                dist = torch.distributions.Normal(mu, sigma)\n",
        "                action = dist.sample()\n",
        "                action_clipped = torch.clamp(action, 0.0, 1.0)\n",
        "                log_prob = dist.log_prob(action)\n",
        "            actions_list.append(action_clipped.item())\n",
        "            step_data.append({\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'log_prob': log_prob,\n",
        "                'agent_id': i\n",
        "            })\n",
        "        actions_np = np.array(actions_list)\n",
        "        next_obs, rewards, done, _ = env.step(actions_np)\n",
        "        episode_reward += np.mean(rewards)\n",
        "        for i in range(num_agents):\n",
        "            agent_memories[i].append((step_data[i]['state'], step_data[i]['action'], step_data[i]['log_prob'], rewards[i], done))\n",
        "        obs = next_obs\n",
        "    all_rewards.append(episode_reward)\n",
        "    if episode % update_interval == 0 and episode > 0:\n",
        "        ppo_agent.buffer = []\n",
        "        for i in range(num_agents):\n",
        "            history = agent_memories[i]\n",
        "            for step in history:\n",
        "                ppo_agent.store(*step)\n",
        "            agent_memories[i] = []\n",
        "        ppo_agent.update()\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode} | Avg team reward: {episode_reward:.2f}\")\n",
        "\n",
        "plt.plot(all_rewards)\n",
        "plt.title(\"MARL PPO training\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg reward\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy inspection",
        "\n",
        "The trained policy runs on a six-step slice to visualize how predicted actions co-vary with the dynamic price signal and ambient temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ppo_agent.policy.eval()\n",
        "episode_length = 6\n",
        "start_step = 0\n",
        "num_agents = env.num_agents\n",
        "price_log = []\n",
        "action_log = []\n",
        "temp_log = []\n",
        "print(f\"Running validation loop for {episode_length} steps...\")\n",
        "for i in range(episode_length):\n",
        "    current_idx = start_step + i\n",
        "    raw_temps = env.temps[current_idx]\n",
        "    raw_cons = env.cons[current_idx]\n",
        "    norm_temps = raw_temps / 30.0\n",
        "    norm_cons = raw_cons / 10.0\n",
        "    states = torch.tensor(np.stack([norm_temps, norm_cons], axis=1), dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        mu, _, _ = ppo_agent.policy(states)\n",
        "        actions = mu.numpy().flatten()\n",
        "    actual_loads = raw_cons * (1 - actions)\n",
        "    total_neighborhood_load = np.sum(actual_loads)\n",
        "    dynamic_price = 0.1 + 0.5 * total_neighborhood_load\n",
        "    for j in range(num_agents):\n",
        "        price_log.append(dynamic_price)\n",
        "        action_log.append(actions[j])\n",
        "        temp_log.append(raw_temps[j])\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(price_log, action_log, c=temp_log, cmap=\"coolwarm\", alpha=0.6, s=50)\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Temperature (C)', rotation=270, labelpad=15)\n",
        "plt.xlabel(\"Dynamic grid price ($/kWh)\")\n",
        "plt.ylabel(\"Optimal load shedding\")\n",
        "plt.title(f\"Smart city policy: price vs. action\\n(5 agents, {episode_length} steps)\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "geo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}